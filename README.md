# RLBerkelyFa2018
Homework of CS 294-112 at UC Berkeley Deep Reinforcement Learning, refer to http://rail.eecs.berkeley.edu/deeprlcourse/


## Summary

| Assignment | Descrirption         |  Code          | Key Points               | Summary   |
|:----------:|:--------------------:|:------------------------:|:------------------------:|:---------:|
| Homework 1 | [Imitation Learning](http://rail.eecs.berkeley.edu/deeprlcourse/static/homeworks/hw1.pdf)|hw1/| $\pi_{\theta}$ as expert, then using as human judgement for imitation learning|           |
| Homework 2 | [Policy Gradient](http://rail.eecs.berkeley.edu/deeprlcourse/static/homeworks/hw2.pdf)|hw2/| refer to hw2/answers/hw2_instructions_answer_by_orlando.tex for math and status summary, also refer to [result](https://github.com/llv22/RLBerkelyFa2018/tree/master/hw2)|           |
| Homework 3 | [Q-Learning and Actor-Critic](http://rail.eecs.berkeley.edu/deeprlcourse/static/homeworks/hw3.pdf)|hw3/| Actor/Critic update of network in [dqn.py](hw3/dqn.py) and using double-Q for value evaluation in the same network in [train_ac_f18.py](hw3/train_ac_f18.py) |           |
